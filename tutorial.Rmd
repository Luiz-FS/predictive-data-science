---
title: "Tutorial Caret"
author: "Luiz Fonseca"
date: "5 de dezembro de 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Caret é a abreviação para Classification And REgression Training.

# Instalação

```{r, eval=FALSE}
install.packages("caret")
```

# A função train()
train() é a função que utilizamos para treinar o modelo. É também uma função núcleo do caret.

```{r}
data(mtcars)
View(mtcars)
?mtcars

library(caret)

# modelo de regressão linear simples
model <- train(mpg ~ wt, 
               data = mtcars,
               method = "lm")

# modelo de regressão linear múltipla
model <- train(mpg ~ ., 
               data = mtcars,
               method = "lm")

# modelo utilizando regressão ridge
model <- train(mpg ~ ., 
               data = mtcars,
               method = "ridge") # pode ser 'lasso'

```
# K-fold cross-validation

O processo de reamostragem pode ser feito usando k-fold cross-validation, leave-one-out cross-validation ou bootstrapping.

```{r}
fitControl <- trainControl(## 10-fold CV
                           method = "cv", # boot", "boot632", "cv", "repeatedcv", "LOOCV", "LGOCV"
                           number = 10,
                           ## repeated ten times
                           repeats = 10)

model.cv <- train(mpg ~ ., 
               data = mtcars,
               method = "lasso",
               trControl = fitControl)

model.cv
```

# Adicionando pre-processamento

```{r}
model.cv <- train(mpg ~ ., 
               data = mtcars,
               method = "lasso",
               trControl = fitControl,
               preProcess = c('scale', 'center')) # default: sem pre-processamento

# Center: subtrai a média do valor 
# scale: normaliza os dados (deixa na mesma escala)

?train
model.cv
```

# Encontrando parâmetros do modelo

Podemos testar vários parâmetros para o modelo através da função expand.grid()

O método ridge tende a aproximar os coeficientes das variáveis preditoras de 0, conforme o lambda aumenta. Isso diminui a flexibilidade do modelo, diminuindo também a variância, porém aumentando o BIAS. A ideia por trás da regressão Ridge é encontrar um lambda que gere um trade-off satisfatório entre BIAS e Variância.

```{r}
lambdaGrid <- expand.grid(lambda = 10^seq(10, -2, length=100))

model.cv <- train(mpg ~ ., 
               data = mtcars,
               method = "ridge",
               trControl = fitControl,
               preProcess = c('scale', 'center'),
               tuneGrid = lambdaGrid,
               na.action = na.omit)   #ignora os NAs

model.cv
```

Podemos utilizar uma busca aleatória de parâmetros com search = "random" no trainControl.

```{r}
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv", 
                           number = 10,
                           repeats = 10,
                           search = "random")  # busca aleatória de hiperparâmetros

model.cv <- train(mpg ~ ., 
               data = mtcars,
               method = "ridge",
               trControl = fitControl,
               preProcess = c('scale', 'center'),
               na.action = na.omit)

model.cv
```

# Importância das variáveis

```{r}
ggplot(varImp(model.cv))
```

# Predições

```{r}
predictions <- predict(model.cv, mtcars)

predictions
```
